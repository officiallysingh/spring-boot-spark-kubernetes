# ===================================================================
# Spring Boot configuration.
#
# This configuration will be overridden by the Spring profile you use,
# for example application-dev.yml if you use the "dev" profile.
#
# Full reference for Standard Spring Boot properties is available at:
# http://docs.spring.io/spring-boot/docs/current/reference/html/common-application-properties.html
# ===================================================================
# set -Dspring.profiles.active=<dev|sit|int> as JVM argument to run in desired profile
# If no profile is specified explicitly, application will fall back to default profile, which is "local"

spring:
  application:
    name: logs-analysis-job
  autoconfigure:
    exclude: org.springframework.boot.autoconfigure.gson.GsonAutoConfiguration
  docker:
    compose:
      enabled: false
  cloud:
    task:
      initialize-enabled: ${ksoot.job.persist}
      external-execution-id: ${ksoot.job.correlation-id}
  main:
    log-startup-info: true

  #------------------------- Kafka configuration -------------------------
  kafka:
    producer:
      bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: org.apache.kafka.common.serialization.StringSerializer
    consumer:
      bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
#      group-id: spark-jobs-group
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      auto-offset-reset: latest

  #------------------------- Postgres configuration -------------------------
  datasource:
    url: ${POSTGRES_URL:jdbc:postgresql://localhost:5432/spark_jobs_db}
    username: ${POSTGRES_USER:postgres}
    password: ${POSTGRES_PASSWORD:admin}
    hikari:
      pool-name: pg-connection-pool
      maximum-pool-size: 32
      minimum-idle: 8
  jpa:
#    defer-datasource-initialization: true
    hibernate:
      ddl-auto: validate
    database: POSTGRESQL
    open-in-view: false
    show-sql: false
    properties:
      '[hibernate.show_sql]': false
      '[hibernate.format_sql]': true
      '[hibernate.use_sql_comments]': true
      '[hibernate.jdbc.time_zone]': UTC
      '[integration.envers.enabled]': true
      '[hibernate.enable_lazy_load_no_trans]': true

#------------------------- Logging configuration -------------------------
logging:
  level:
    ROOT: info
    '[org.mongodb.driver]': warn
    '[org.apache.spark]': warn
    '[org.apache.hadoop]': warn
    '[org.sparkproject]': warn
    '[com.mongodb.spark.sql.connector.read.partitioner.Partitioner]': warn
    '[org.apache.kafka.clients.admin.KafkaAdminClient]': error
    '[org.apache.kafka.clients]': error
debug: false

# ===================================================================
# Application specific properties
# Add your own application properties here
# ===================================================================

#------------------------- Job configurations -------------------------
ksoot:
  job:
    correlation-id: ${CORRELATION_ID:${spring.application.name}-1}
    persist: ${PERSIST_JOB:false}
    job-stop-topic: ${JOB_STOP_TOPIC:job-stop-requests}
  connector:
    save-mode: Append
    output-mode: Update
    jdbc-options:
      url: ${JDBC_URL:jdbc:postgresql://localhost:5432}
      database: ${JDBC_DB:error_logs_db}
      username: ${JDBC_USER:postgres}
      password: ${JDBC_PASSWORD:admin}
      batchsize: ${JDBC_BATCH_SIZE:1000}
      isolation-level: ${JDBC_ISOLATION_LEVEL:READ_UNCOMMITTED}
    file-options:
      format: csv
      header: true
      path: ${SPARK_OUTPUT_PATH:spark-space/output}
      merge: true
    kafka-options:
      bootstrap-servers: ${KAFKA_BOOTSTRAP_SERVERS:localhost:9092}
      topic: ${KAFKA_ERROR_LOGS_TOPIC:error-logs}
      fail-on-data-loss: ${KAFKA_FAIL_ON_DATA_LOSS:false}

#------------------------- Spark configurations -------------------------
spark:
  app.name: ${spring.application.name}
  master: k8s://https://kubernetes.default.svc
  driver:
    memory: 2g
    cores: 2
  executor:
    instances: 2
    memory: 2g
    cores: 2
  ui:
    enabled: true
  streaming:
    stopGracefullyOnShutdown: true
  sql:
    streaming:
      checkpointLocation: ${CHECKPOINT_LOCATION:spark-space/checkpoints}
      forceDeleteTempCheckpointLocation: true
    adaptive:
      enabled: true
  checkpoint:
    compress: true
  hadoop:
    fs:
      s3a:
        access.key: ${AWS_ACCESS_KEY:<put your aws access key>}
        secret.key: ${AWS_SECRET_KEY:<put your aws secret key>}
        endpoint: ${AWS_S3_ENDPOINT:<put your aws s3 endpoint>}
        impl: org.apache.hadoop.fs.s3a.S3AFileSystem
        path.style.access: true  # For path-style access, useful in some S3-compatible services
        connection.ssl.enabled: true  # Enable SSL
        fast.upload: true  # Enable faster uploads
    google.cloud.auth.service.account.json.keyfile: ${GCS_KEY_FILE:<put your sa-key.json>}



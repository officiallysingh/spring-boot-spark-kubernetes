{
  "properties": [
    {
      "name": "ksoot.hadoop-dll",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Hadoop dll file path. Required only while running the job on <b>Windows machine</b>."
    },
    {
      "name": "ksoot.job.correlationId",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Unique correlation id for each Job execution."
    },
    {
      "name": "ksoot.job.persist",
      "type": "java.lang.Boolean",
      "defaultValue": "false",
      "description": "Whether to persist Jobs status in Database."
    },
    {
      "name": "ksoot.job.job-stop-topic",
      "type": "java.lang.String",
      "defaultValue": "job-stop-requests",
      "description": "Kafka topic name to listen to receive Job stop requests."
    },
    {
      "name": "ksoot.connector.save-mode",
      "type": "java.lang.String",
      "defaultValue": "Overwrite",
      "description": "To specify the expected behavior of saving a DataFrame to a data source. Applicable in Spark Batch jobs only."
    },
    {
      "name": "ksoot.connector.output-mode",
      "type": "java.lang.String",
      "defaultValue": "Append",
      "description": "Describes what data will be written to a streaming sink when there is new data available in a streaming Dataset. Applicable in Spark Streaming jobs only."
    },
    {
      "name": "ksoot.connector.file-options.format",
      "type": "java.lang.String",
      "defaultValue": "csv",
      "description": "Input file format. 'csv' or 'parquet'."
    },
    {
      "name": "ksoot.connector.file-options.header",
      "type": "java.lang.Boolean",
      "defaultValue": true,
      "description": "Whether to consider first row as as header row in an input file. Column names would be the columns in Spark dataset."
    },
    {
      "name": "ksoot.connector.file-options.merge",
      "type": "java.lang.String",
      "defaultValue": "false",
      "description": "Whether to merge output in single file."
    },
    {
      "name": "ksoot.connector.file-options.path",
      "type": "java.lang.String",
      "defaultValue": "false",
      "description": "Output path where output will be saved."
    },
    {
      "name": "ksoot.connector.jdbc-options.url",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Database url."
    },
    {
      "name": "ksoot.connector.jdbc-options.driver",
      "type": "java.lang.String",
      "defaultValue": "org.postgresql.Driver",
      "description": "Database driver name."
    },
    {
      "name": "ksoot.connector.jdbc-options.database",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Ingestion Database name."
    },
    {
      "name": "ksoot.connector.jdbc-options.username",
      "type": "java.lang.String",
      "defaultValue": "postgres",
      "description": "Database username."
    },
    {
      "name": "ksoot.connector.jdbc-options.password",
      "type": "java.lang.String",
      "defaultValue": "admin",
      "description": "Database password."
    },
    {
      "name": "ksoot.connector.jdbc-options.fetchsize",
      "type": "java.lang.Integer",
      "defaultValue": "1000",
      "description": "The JDBC fetch size, which determines how many rows to fetch per round trip. Applicable only while reading. This can help performance on JDBC drivers which default to low fetch size (e.g. Oracle with 10 rows)."
    },
    {
      "name": "ksoot.connector.jdbc-options.batchsize",
      "type": "java.lang.Integer",
      "defaultValue": "1000",
      "description": "In spark sql this option decide how many rows to insert per round trip. Applicable only while writing."
    },
    {
      "name": "ksoot.connector.jdbc-options.isolation-level",
      "type": "java.lang.String",
      "defaultValue": "READ_UNCOMMITTED",
      "description": "The transaction isolation level, which applies to current connection. Applicable only while writing. It can be one of NONE, READ_COMMITTED, READ_UNCOMMITTED, REPEATABLE_READ, or SERIALIZABLE, corresponding to standard transaction isolation levels defined by JDBC's Connection object, with default of READ_UNCOMMITTED. Please refer the documentation in java.sql.Connection."
    },
    {
      "name": "ksoot.connector.mongo-options.url",
      "type": "java.lang.String",
      "defaultValue": "mongodb://localhost:27017",
      "description": "MongoDB connection url."
    },
    {
      "name": "ksoot.connector.mongo-options.database",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "MongoDB database name."
    },
    {
      "name": "ksoot.connector.arango-options.endpoints",
      "type": "java.util.List<java.lang.String>",
      "defaultValue": "localhost:8529",
      "description": "ArangoDB connection urls."
    },
    {
      "name": "ksoot.connector.arango-options.database",
      "type": "java.lang.String",
      "defaultValue": "_system",
      "description": "ArangoDB database name."
    },
    {
      "name": "ksoot.connector.arango-options.username",
      "type": "java.lang.String",
      "defaultValue": "root",
      "description": "ArangoDB username."
    },
    {
      "name": "ksoot.connector.arango-options.password",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "ArangoDB password."
    },
    {
      "name": "ksoot.connector.arango-options.sslEnabled",
      "type": "java.lang.Boolean",
      "defaultValue": "",
      "description": "Whether to enable SSL."
    },
    {
      "name": "ksoot.connector.arango-options.sslCertValue",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Ssl cert value."
    },
    {
      "name": "ksoot.connector.arango-options.cursor-ttl",
      "type": "java.lang.String",
      "defaultValue": "PT30S",
      "description": "Cursor ttl."
    },
    {
      "name": "ksoot.connector.kafka-options.bootstrap-servers",
      "type": "java.lang.String",
      "defaultValue": "localhost:9092",
      "description": "Kafka Bootstrap servers."
    },
    {
      "name": "ksoot.connector.kafka-options.topic",
      "type": "java.lang.String",
      "defaultValue": "",
      "description": "Kafka topic where to write output."
    },
    {
      "name": "ksoot.connector.kafka-options.key-serializer",
      "type": "java.lang.String",
      "defaultValue": "org.apache.kafka.common.serialization.StringSerializer",
      "description": "Kafka Key serializer type."
    },
    {
      "name": "ksoot.connector.kafka-options.value-serializer",
      "type": "java.lang.String",
      "defaultValue": "org.apache.kafka.common.serialization.ByteArraySerializer",
      "description": "Kafka val serializer type."
    },
    {
      "name": "ksoot.connector.kafka-options.fail-on-data-loss",
      "type": "java.lang.Boolean",
      "defaultValue": "true",
      "description": "Whether to fail the query when it's possible that data is lost (e.g., topics are deleted, or offsets are out of range). This may be a false alarm. You can disable it when it doesn't work as you expected."
    },
    {
      "name": "ksoot.connector.kafka-options.starting-offsets",
      "type": "java.lang.String",
      "defaultValue": "latest",
      "description": "The start point when a query is started, either \"earliest\" which is from the earliest offsets, \"latest\" which is just from the latest offsets, or a json string specifying a starting offset for each TopicPartition. In the json, -2 as an offset can be used to refer to earliest, -1 to latest. Note: For batch queries, latest (either implicitly or by using -1 in json) is not allowed. For streaming queries, this only applies when a new query is started, and that resuming will always pick up from where the query left off. Newly discovered partitions during a query will start at earliest."
    }
  ],
  "hints": [
    {
      "name": "ksoot.connector.file-options.format",
      "values": [
        {
          "value": "csv",
          "description": "File format csv."
        },
        {
          "value": "parquet",
          "description": "File format parquet."
        },
        {
          "value": "avro",
          "description": "File format avro."
        },
        {
          "value": "text",
          "description": "File format text."
        },
        {
          "value": "orc",
          "description": "File format avro."
        }
      ]
    },
    {
      "name": "ksoot.connector.save-mode",
      "values": [
        {
          "value": "Append",
          "description": "Append mode means that when saving a DataFrame to a data source, if data/ table already exists, contents of the DataFrame are expected to be appended to existing data"
        },
        {
          "value": "Overwrite",
          "description": "Overwrite mode means that when saving a DataFrame to a data source, if data/ table already exists, existing data is expected to be overwritten by the contents of the DataFrame."
        },
        {
          "value": "ErrorIfExists",
          "description": "ErrorIfExists mode means that when saving a DataFrame to a data source, if data already exists, an exception is expected to be thrown."
        },
        {
          "value": "Ignore",
          "description": "Ignore mode means that when saving a DataFrame to a data source, if data already exists, the save operation is expected to not save the contents of the DataFrame and to not change the existing data."
        }
      ]
    },
    {
      "name": "ksoot.connector.output-mode",
      "values": [
        {
          "value": "Append",
          "description": "This is the default mode, where only the new rows added to the Result Table since the last trigger will be outputted to the sink. This is supported for only those queries where rows added to the Result Table is never going to change. Hence, this mode guarantees that each row will be output only once (assuming fault-tolerant sink). For example, queries with only select, where, map, flatMap, filter, join, etc. will support Append mode."
        },
        {
          "value": "Complete",
          "description": "The whole Result Table will be outputted to the sink after every trigger. This is supported for aggregation queries."
        },
        {
          "value": "Update",
          "description": "Only the rows in the Result Table that were updated since the last trigger will be outputted to the sink."
        }
      ]
    },
    {
      "name": "ksoot.connector.jdbc-options.isolation-level",
      "values": [
        {
          "value": "READ_UNCOMMITTED",
          "description": "Dirty reads, non-repeatable reads, and phantom reads can occur."
        },
        {
          "value": "READ_COMMITTED",
          "description": "Dirty reads are prevented; non-repeatable reads and phantom reads can occur."
        },
        {
          "value": "READ_UNCOMMITTED",
          "description": "Dirty reads, non-repeatable reads, and phantom reads can occur. "
        },
        {
          "value": "REPEATABLE_READ",
          "description": "Dirty reads and non-repeatable reads are prevented; phantom reads can occur."
        },
        {
          "value": "SERIALIZABLE",
          "description": "Dirty reads, non-repeatable reads and phantom reads are prevented."
        }
      ]
    },
    {
      "name": "ksoot.connector.kafka-options.fail-on-data-loss",
      "values": [
        {
          "value": "true",
          "description": "The streaming query will fail if data is lost. This can happen if the topic is deleted or recreated, or if data is aged out by Kafka."
        },
        {
          "value": "false",
          "description": "The query will continue running, even if data is lost. However, this doesn't solve the underlying issue of potential data loss."
        }
      ]
    }
  ]
}

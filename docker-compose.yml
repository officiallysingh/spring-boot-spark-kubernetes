services:
#  mongo:
#    image: mongo:latest
#    container_name: mongo
#    command: [ "--replSet", "rs0", "--bind_ip_all", "--port", "27017" ]
#    ports:
#      - "27017:27017"
#    extra_hosts:
#      - "host.docker.internal:host-gateway"
#    healthcheck:
#      test: echo "try { rs.status() } catch (err) { rs.initiate({_id:'rs0',members:[{_id:0,host:'host.docker.internal:27017'}]}) }" | mongosh --port 27017 --quiet
#      interval: 5s
#      timeout: 30s
#      start_period: 0s
#      retries: 30
#    volumes:
#      - "mongo_data:/data/db"
#      - "mongo_config:/data/configdb"
#    networks:
#      - ksoot

  arango:
    image: arangodb/arangodb:latest
    container_name: arango
    environment:
      ARANGO_ROOT_PASSWORD: admin
      ARANGO_DB_NAME: products_db
    ports:
      - "8529:8529"
    volumes:
      - arango_data:/var/lib/arangodb3
    networks:
      - ksoot

#  postgres:
#    image: postgres:latest
#    hostname: postgres
#    container_name: postgres
#    environment:
#      POSTGRES_DB: spark_jobs_db
#      POSTGRES_USER: postgres
#      POSTGRES_PASSWORD: admin
#    ports:
#      - "5432:5432"
#    volumes:
#      - postgres_data:/var/lib/postgresql/data
#    networks:
#      - ksoot

  zookeeper:
    image: confluentinc/cp-zookeeper:7.3.2
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_SERVER_ID: 1
      ZOOKEEPER_SERVERS: zookeeper:2888:3888
    networks:
      - ksoot

  kafka:
    image: confluentinc/cp-kafka:7.3.2
    hostname: kafka
    container_name: kafka
    ports:
      - "9092:9092"
      - "29092:29092"
      - "9999:9999"
    environment:
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:19092,EXTERNAL://${DOCKER_HOST_IP:-127.0.0.1}:9092,DOCKER://host.docker.internal:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT,DOCKER:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      KAFKA_ZOOKEEPER_CONNECT: "zookeeper:2181"
      KAFKA_BROKER_ID: 1
      KAFKA_LOG4J_LOGGERS: "kafka.controller=INFO,kafka.producer.async.DefaultEventHandler=INFO,state.change.logger=INFO"
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_JMX_PORT: 9999
      KAFKA_JMX_HOSTNAME: ${DOCKER_HOST_IP:-127.0.0.1}
      KAFKA_AUTHORIZER_CLASS_NAME: kafka.security.authorizer.AclAuthorizer
      KAFKA_ALLOW_EVERYONE_IF_NO_ACL_FOUND: "true"
    depends_on:
      - zookeeper
    networks:
      - ksoot

  kafka-ui:
    container_name: kafka-ui
    image: provectuslabs/kafka-ui:latest
    ports:
      - 8100:8080
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_ZOOKEEPER: "zookeeper:2181"
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: "kafka:29092"
      KAFKA_BROKERCONNECT: "kafka:29092"
      KAFKA_CLUSTERS_0_METRICS_PORT: 9997
      DYNAMIC_CONFIG_ENABLED: 'true'
    depends_on:
      - kafka
    networks:
      - ksoot

  prometheus:
    container_name: prometheus
    image: prom/prometheus:v3.0.1 # https://hub.docker.com/r/prom/prometheus
    extra_hosts: ['host.docker.internal:host-gateway']
    command:
      - --enable-feature=exemplar-storage
      - --web.enable-remote-write-receiver
      - --config.file=/etc/prometheus/prometheus.yml
    volumes:
      - prometheus_data:/prometheus
      - ./docker/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
  push-gateway:
    container_name: push-gateway
    image: prom/pushgateway:v1.10.0
    extra_hosts: ['host.docker.internal:host-gateway']
    ports:
      - "9091:9091"
    networks:
      - ksoot
  grafana:
    container_name: grafana
    image: grafana/grafana:11.4.0 # https://hub.docker.com/r/grafana/grafana/tags and https://github.com/grafana/grafana/releases
    extra_hosts: ['host.docker.internal:host-gateway']
    environment:
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_DISABLE_LOGIN_FORM=true
    volumes:
      - ./docker/grafana/grafana.ini:/etc/grafana/grafana.ini:ro
      - ./docker/grafana/provisioning/datasources:/etc/grafana/provisioning/datasources:ro
      - ./docker/grafana/provisioning/dashboards:/etc/grafana/provisioning/dashboards:ro
#      - ./docker/grafana/provisioning/alerting:/etc/grafana/provisioning/alerting:ro
    ports:
      - "3000:3000"
    networks:
      - ksoot
  tempo-init:
    # Tempo runs as user 10001, and docker compose creates the volume as root.
    # As such, we need to chown the volume in order for Tempo to start correctly.
    # This should not be needed but this is the official solution recommended by Tempo maintainers
    # See: https://github.com/grafana/tempo/blob/a21001a72a5865bfcfc1b0d2dfa30160c5a26103/example/docker-compose/local/docker-compose.yaml
    # See: https://github.com/grafana/tempo/issues/1657
    image: &tempoImage grafana/tempo:2.6.1 # https://hub.docker.com/r/grafana/tempo/tags and https://github.com/grafana/tempo/releases
    user: root
    entrypoint:
      - "chown"
      - "10001:10001"
      - "/var/tempo"
    volumes:
      - tempo_data:/var/tempo
    networks:
      - ksoot
  tempo:
    container_name: tempo
    image: *tempoImage
    extra_hosts: ['host.docker.internal:host-gateway']
    command: ['-config.file=/etc/tempo.yml']
    depends_on: ['tempo-init']
    volumes:
      - tempo_data:/var/tempo
      - ./docker/grafana/tempo.yml:/etc/tempo.yml:ro
    ports:
      - "3200:3200"    # tempo
      - "9411:9411"    # zipkin
    networks:
      - ksoot
  loki:
    container_name: loki
    image: grafana/loki:3.0.1 # https://hub.docker.com/r/grafana/loki/tags and https://github.com/grafana/loki/releases
    extra_hosts: ['host.docker.internal:host-gateway']
    command: ['-config.file=/etc/loki/local-config.yaml']
    ports:
      - "3100:3100"
    networks:
      - ksoot
  promtail:
    container_name: promtail
    image: grafana/promtail:3.0.1
    extra_hosts: ['host.docker.internal:host-gateway']
    volumes:
      - /var/log:/var/log
    command: -config.file=/etc/promtail/config.yml
    networks:
      - ksoot

#  otel-collector:
#    image: otel/opentelemetry-collector-contrib
#    volumes:
#      - ./otel-collector-config.yaml:/etc/otelcol-contrib/config.yaml
#    ports:
#      - 1888:1888 # pprof extension
#      - 8888:8888 # Prometheus metrics exposed by the Collector
#      - 8889:8889 # Prometheus exporter metrics
#      - 13133:13133 # health_check extension
#      - 4317:4317 # OTLP gRPC receiver
#      - 4318:4318 # OTLP http receiver
#      - 55679:55679 # zpages extension

volumes:
#  mongo_data:
  mongo_config:
  arango_data:
#  postgres_data:
  conduktor_data:
  prometheus_data:
  tempo_data:

networks:
  ksoot:
    driver: bridge
